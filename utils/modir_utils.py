import os
import sys
import csv
import numpy as np
import faiss
import torch
import torch.distributed as dist
from torch.utils.data import DataLoader
try:
    from apex import amp
except ImportError:
    print("apex not imported")

from utils.util import (
    is_first_worker, 
    StreamingDataset,
    EmbeddingCache,
)
from model.domain_classifier import DomainClassifier, dry_dc_evaluation
from drivers.run_ann_data_gen import StreamInferenceDoc, EvalDevQuery
from data.msmarco_data import GetProcessingFn

import logging
logger = logging.getLogger(__name__)


def compute_total_grad_L2_norm(param_list):
    total_norm = 0.0
    for p in param_list:
        if p.grad is not None:
            total_norm += torch.norm(p.grad) ** 2
    total_norm = total_norm ** 0.5
    return total_norm.item()


def intrain_dev_eval(args, global_step, model, tb_writer, prev_dry_dc_state_dict,
                     all_datasets=False):
    model.eval()

    query_embs = []
    passage_embs = []

    intraindev_data_name = args.intraindev_data_name[:2]
    with amp.disable_casts():  # back to fp32
        for i_dev, data_name in enumerate(intraindev_data_name):
            data_path = args.intraindev_data_dir[i_dev]
            dev_evaluation_results = dev_evaluation(
                args, data_path, model, return_embs=True)
            if is_first_worker():
                # query/passage embs obtained by the first worker
                # are actually generated by all workers
                # see the implementation of StreamInferenceDoc()
                dev_result_dict, (query_emb, passage_emb, query_emb2id, passage_emb2id) = dev_evaluation_results

                if i_dev <= 1:
                    query_embs.append(query_emb)
                    passage_embs.append(passage_emb)
                    np.save(
                        os.path.join(args.saved_embedding_dir, f"{data_name}_query-step{global_step}.npy"),
                        query_emb
                    )
                    np.save(
                        os.path.join(args.saved_embedding_dir, f"{data_name}_passage-step{global_step}.npy"),
                        passage_emb
                    )
                    np.save(
                        os.path.join(args.saved_embedding_dir, f"{data_name}_query2id-step{global_step}.npy"),
                        query_emb2id
                    )
                    np.save(
                        os.path.join(args.saved_embedding_dir, f"{data_name}_passage2id-step{global_step}.npy"),
                        passage_emb2id
                    )
            
                if i_dev == 1:
                    dry_dc_model = DomainClassifier(args)
                    dry_dc_model.to(args.device)
                    dry_dc_acc, prev_dry_dc_acc, prev_dry_dc_state_dict = dry_dc_evaluation(
                        args, dry_dc_model, query_embs, passage_embs, prev_dry_dc_state_dict)
                    tb_writer.add_scalar("dc_acc_dry_Q", float(dry_dc_acc[0]), global_step)
                    tb_writer.add_scalar("dc_acc_dry_P", float(dry_dc_acc[1]), global_step)
                    if prev_dry_dc_acc[0] is not None:
                        tb_writer.add_scalar("dc_acc_prev_dry_Q", float(prev_dry_dc_acc[0]), global_step)
                        tb_writer.add_scalar("dc_acc_prev_dry_P", float(prev_dry_dc_acc[1]), global_step)
                    del dry_dc_model
                    torch.cuda.empty_cache()
                print(data_name, dev_result_dict)
                for k, v in dev_result_dict.items():
                    tb_writer.add_scalar(f"{data_name}-{k}", v, global_step)
            if args.local_rank != -1:
                dist.barrier()
    model.train()
    return prev_dry_dc_state_dict


def intrain_save_checkpoint(args, global_step,
                            model, tokenizer, optimizer, scheduler):
    if is_first_worker():
        # identical with the one from original_drivers/run_ann
        output_dir = os.path.join(
            args.output_dir, "checkpoint-{}".format(global_step))
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
        model_to_save = (
            model.module if hasattr(model, "module") else model
        )  # Take care of distributed/parallel training
        model_to_save.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)

        torch.save(args, os.path.join(output_dir, "training_args.bin"))
        logger.info("Saving model checkpoint to %s", output_dir)

        torch.save(
            optimizer.state_dict(),
            os.path.join(
                output_dir,
                "optimizer.pt"))
        torch.save(
            scheduler.state_dict(),
            os.path.join(
                output_dir,
                "scheduler.pt"))
        logger.info(
            "Saving optimizer and scheduler states to %s",
            output_dir)
    if args.local_rank != -1:
        dist.barrier()



def build_input_from_batch(args, batch, mode='full', triplet=False):
    if triplet:  # from ann
        if mode == 'query':
            inputs = {
                "input_ids": batch[0].long(),
                "attention_mask": batch[1].long()
            }
        elif mode == 'pos_doc':
            inputs = {
                "input_ids": batch[3].long(),
                "attention_mask": batch[4].long()
            }
        elif mode == 'neg_doc':
            inputs = {
                "input_ids": batch[6].long(),
                "attention_mask": batch[7].long()
            }
        else:
            inputs = {
                "query_ids": batch[0].long(),
                "attention_mask_q": batch[1].long(),
                "input_ids_a": batch[3].long(),
                "attention_mask_a": batch[4].long(),
                "input_ids_b": batch[6].long(),
                "attention_mask_b": batch[7].long()
            }
    else:  # from raw data
        if mode == 'query':
            inputs = {
                "input_ids": batch[0].long(),
                "attention_mask": batch[1].long()
            }
        elif mode == 'pos_doc':
            inputs = {
                "input_ids": batch[2].long(),
                "attention_mask": batch[3].long()
            }
        elif mode == 'neg_doc':
            inputs = {
                "input_ids": batch[4].long(),
                "attention_mask": batch[5].long()
            }
        elif mode == 'full':
            inputs = {
                "query_ids": batch[0].long(),
                "attention_mask_q": batch[1].long(),
                "input_ids_a": batch[2].long(),
                "attention_mask_a": batch[3].long(),
                "input_ids_b": batch[4].long(),
                "attention_mask_b": batch[5].long()
            }
    return inputs


def get_module(model):
    return model.module if hasattr(model, "module") else model


def build_dl_iter_from_file(args, file_obj, process_fn):
    file_obj.seek(0)
    sds = StreamingDataset(file_obj, process_fn)
    dataloader = DataLoader(sds, batch_size=args.per_gpu_train_batch_size, num_workers=0)
    iterator = iter(dataloader)
    return dataloader, iterator


def get_next(iterator, args, file_obj, process_fn, batch_size):
    try:
        batch = next(iterator)
        assert batch_size == batch[0].shape[0]
    except (AssertionError, StopIteration):
        # print('Build new iterator')
        _, iterator = build_dl_iter_from_file(args, file_obj, process_fn)
        batch = next(iterator)
    return batch, iterator


def dev_evaluation(args, data_path, model,
                   return_embs=False):
    logger.info("Loading dev query_2_pos_docid")
    dev_query_positive_id = {}
    query_positive_id_path = os.path.join(data_path, "dev-qrel.tsv")
    with open(query_positive_id_path, 'r', encoding='utf8') as f:
        tsvreader = csv.reader(f, delimiter="\t")
        for [topicid, docid, rel] in tsvreader:
            topicid = int(topicid)
            docid = int(docid)
            if topicid not in dev_query_positive_id:
                dev_query_positive_id[topicid] = {}
            dev_query_positive_id[topicid][docid] = max(0, int(rel))

    old_max_seq_length = args.max_seq_length
    args.max_seq_length = 512  # otherwise it crashes
    args.rank = args.local_rank

    old_max_query_length = args.max_query_length
    if 'arguana' in data_path:
        args.max_query_length = 512

    dev_tmp_ann_data_dir = "../dev_tmp_ann_data"
    os.makedirs(dev_tmp_ann_data_dir, exist_ok=True)

    logger.info("***** inference of dev query *****")
    dev_query_collection_path = os.path.join(data_path, "dev-query")
    dev_query_cache = EmbeddingCache(dev_query_collection_path)
    with dev_query_cache as emb:
        dev_query_embedding, dev_query_embedding2id = StreamInferenceDoc(
            args,
            model,
            GetProcessingFn(args, query=True), 
            "dev_query_0_",
            emb,
            output_path=dev_tmp_ann_data_dir,
            is_query_inference=True)

    logger.info("***** inference of dev passages *****")
    dev_passage_collection_path = os.path.join(data_path, "dev-passages")
    dev_passage_cache = EmbeddingCache(dev_passage_collection_path)
    with dev_passage_cache as emb:
        dev_passage_embedding, dev_passage_embedding2id = StreamInferenceDoc(
            args,
            model,
            GetProcessingFn(args, query=False),
            "dev_passage_0_",
            emb,
            output_path=dev_tmp_ann_data_dir,
            is_query_inference=False)

    args.max_seq_length = old_max_seq_length
    args.max_query_length = old_max_query_length
    torch.cuda.empty_cache()
    if is_first_worker():
        # ANN search for dev passages and dev queries
        dev_dim = dev_passage_embedding.shape[1]
        print('dev passage embedding shape: ' + str(dev_passage_embedding.shape))
        faiss.omp_set_num_threads(16)
        dev_cpu_index = faiss.IndexFlatIP(dev_dim)
        dev_cpu_index.add(dev_passage_embedding)
        logger.info("***** Done Dev ANN Index *****")

        _, dev_I = dev_cpu_index.search(dev_query_embedding, 100)  # I: [number of queries, topk]
        result_dict, num_queries_dev = EvalDevQuery(
            args, dev_query_embedding2id, dev_passage_embedding2id,
            dev_query_positive_id, dev_I)
    
        if return_embs:
            return (result_dict,
                    (dev_query_embedding, dev_passage_embedding, dev_query_embedding2id, dev_passage_embedding2id))
        else:
            return result_dict
